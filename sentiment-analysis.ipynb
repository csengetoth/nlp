{"cells":[{"metadata":{},"cell_type":"markdown","source":"[Thanks for useful notebook.](https://www.kaggle.com/michawilkosz/twitter-sentiment-analysis-using-tensorflow#Model-test-harness)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv', \n                   encoding = 'latin', header=None)\n\ndata = data.rename(columns={0: 'target', 1: 'id', 2: 'date', 3: 'query', 4: 'username', 5: 'content'})\n\ndata['target'] = data['target'].replace([0, 4],['Negative','Positive'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['length'] = data.content.str.split().apply(len)\n\ndata['length'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['length'].quantile([0.90,0.95,0.975,0.995])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['id','date','query','username','length'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.target = data.target.replace({'Positive': 1, 'Negative': 0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nenglish_stopwords = stopwords.words('english')\nstemmer = SnowballStemmer('english')\nregex = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\ndef preprocess(content, stem=False):\n    content = re.sub(regex, ' ', str(content).lower()).strip()\n    tokens = []\n    for token in content.split():\n        if token not in english_stopwords:\n            tokens.append(stemmer.stem(token))\n    return \" \".join(tokens)\n\ndata.content = data.content.apply(lambda x: preprocess(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val =  train_test_split(data.content,data.target, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.scripts.glove2word2vec import glove2word2vec\nfrom gensim.models import KeyedVectors\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nglove_file1='../input/glove6b100dtxt/glove.6B.100d.txt'\nword2vec_output_file1 = 'glove.6B.100d.txt.word2vec'\nglove_loaded1=glove2word2vec(glove_file1, word2vec_output_file1)\nembeddings_dictionary1 = KeyedVectors.load_word2vec_format(word2vec_output_file1, binary=False)\n        \nglove_file2='../input/glove6b50dtxt/glove.6B.50d.txt'\nword2vec_output_file2 = 'glove.6B.50d.txt.word2vec'\nglove_loaded2=glove2word2vec(glove_file2, word2vec_output_file2)\nembeddings_dictionary2 = KeyedVectors.load_word2vec_format(word2vec_output_file2, binary=False)\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nvocab_size = len(tokenizer.word_index) + 1\n\nembeddings_matrix1 = np.zeros((vocab_size, 100))\nfor word, index in tokenizer.word_index.items():\n    if word in embeddings_dictionary1.index2entity:\n        embedding_vector = embeddings_dictionary1[word]\n    else:\n        embedding_vector = None\n    if embedding_vector is not None:\n        embeddings_matrix1[index] = embedding_vector\n        \nembeddings_matrix2 = np.zeros((vocab_size, 50))\nfor word, index in tokenizer.word_index.items():\n    if word in embeddings_dictionary2.index2entity:\n        embedding_vector = embeddings_dictionary2[word]\n    else:\n        embedding_vector = None\n    if embedding_vector is not None:\n        embeddings_matrix2[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.utils.multiclass import unique_labels\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM\nfrom keras.optimizers import Adam\nimport tensorflow as tf\n\n\nclass TemplateClassifier(BaseEstimator, ClassifierMixin):\n\n    def __init__(self,vocab_size=0, max_length=50, epochs = 10, embedding = 'glove100d'):\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.epochs = epochs\n        self.embedding = embedding\n        \n    def fit(self, X, y):\n        self.classes_ = unique_labels(y)\n        self.X_ = X\n        self.y_ = y\n        \n        X_train, X_val, y_train, y_val =  train_test_split(X, y, test_size=0.1)\n        \n        if self.vocab_size != 0:\n            self.tokenizer = Tokenizer(num_words = self.vocab_size)\n            self.tokenizer.fit_on_texts(X_train)\n            self.vocab_size = self.vocab_size + 1\n        else:\n            self.tokenizer = Tokenizer()\n            self.tokenizer.fit_on_texts(X_train)\n            self.vocab_size = len(self.tokenizer.word_index) + 1\n        \n        sequences_train = self.tokenizer.texts_to_sequences(X_train)  \n        sequences_val = self.tokenizer.texts_to_sequences(X_val) \n        X_train = pad_sequences(sequences_train, maxlen=self.max_length, padding='post')\n        X_val = pad_sequences(sequences_val, maxlen=self.max_length, padding='post')\n        \n        from gensim.scripts.glove2word2vec import glove2word2vec\n        from gensim.models import KeyedVectors\n        \n        if self.embedding == 'glove100d':\n            embeddings_dictionary = embeddings_dictionary1\n            embedding_dim = 100\n            embeddings_matrix = embeddings_matrix1[:self.vocab_size,]\n\n        elif self.embedding == 'glove50d':\n            embeddings_dictionary = embeddings_dictionary2\n            embedding_dim = 50\n            embeddings_matrix = embeddings_matrix2[:self.vocab_size,]\n        \n        embedding_layer = tf.keras.layers.Embedding(self.vocab_size, embedding_dim, \n                                                    input_length=self.max_length, \n                                                    weights=[embeddings_matrix], trainable=False)\n    \n        self.model = Sequential([\n            embedding_layer,\n            tf.keras.layers.Bidirectional(LSTM(128, return_sequences=True)),\n            tf.keras.layers.Bidirectional(LSTM(128)),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(1, activation='sigmoid'),\n            ])\n        self.model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001),\n                      metrics=['accuracy'])\n        \n        self.model.fit(X_train, y_train, epochs = self.epochs, batch_size = 1000,\n              validation_data=(X_val, y_val),\n              callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n\n    def predict(self, X):\n        # Check is fit had been called\n        check_is_fitted(self)\n     \n        sequences_train = self.tokenizer.texts_to_sequences(X)  \n        X_val = pad_sequences(sequences_train, maxlen=self.max_length, padding='post')\n\n        y = self.model.predict(X_val)\n        \n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nclass TemplateClassifier2(BaseEstimator, ClassifierMixin):\n\n    #def __init__(self):\n        \n    def fit(self, X, y):\n        self.classes_ = unique_labels(y)\n        self.X_ = X\n        self.y_ = y\n        \n        self.vectorizer = TfidfVectorizer()\n        X = self.vectorizer.fit_transform(self.X_)\n        \n        self.classifier = LogisticRegression()\n        self.classifier.fit(X,y)\n\n    def predict(self, X):\n        # Check is fit had been called\n        check_is_fitted(self)\n    \n        X = self.vectorizer.transform(X)  \n        y = self.classifier.predict(X)\n        \n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nestimators = [\n    ('g100d', TemplateClassifier(vocab_size=1000, max_length=30 ,embedding = 'glove100d')),\n    ('g50d', TemplateClassifier(max_length=20, embedding = 'glove50d')),\n    ('g50d2', TemplateClassifier(vocab_size=500, max_length=20, embedding = 'glove50d')),\n    ('lr', TemplateClassifier2())\n]\n\nreg = StackingClassifier(\n     estimators=estimators,\n     final_estimator=GradientBoostingClassifier(),\n     cv=2\n)\n\nreg.fit(X_train,y_train)\n\nfinal_predictions = reg.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(final_predictions == y_val)/y_val.shape[0]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}